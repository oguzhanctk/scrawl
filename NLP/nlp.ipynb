{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\oguzh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\oguzh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk as nlp\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "nlp.download(\"wordnet\")\n",
    "nlp.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19953"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter = pd.read_csv(\"gender-classifier-DFE-791531.csv\", encoding=\"latin1\")\n",
    "X = twitter.loc[:, [\"gender\", \"text\"]]\n",
    "X.dropna(axis=0, inplace=True)\n",
    "y = [1 if x == \"male\" else 0 for x in X.gender]\n",
    "X.head()\n",
    "len(X.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19953"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        robbie e responds to critic after win against ...\n",
       "1        it felt like they were my friend and i wa livi...\n",
       "2        i absolutely adore when louis start the song i...\n",
       "3        hi jordanspieth looking at the url do you use ...\n",
       "4        watching neighbour on sky catching up with the...\n",
       "                               ...                        \n",
       "20045    lookupondeath fine and i ll drink tea too i lo...\n",
       "20046    greg hardy you a good player and all but don t...\n",
       "20047    you can miss people and still never want to se...\n",
       "20048    bitemyapp i had noticed your tendency to pee o...\n",
       "20049    i think for my apush creative project i m goin...\n",
       "Name: text, Length: 19953, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepareData(data):\n",
    "    series_to_array = data.values\n",
    "    wnl = nlp.WordNetLemmatizer() \n",
    "    for i, v in enumerate(series_to_array):\n",
    "        temp = re.sub(\"[^a-zA-z]\", \" \", v).lower()\n",
    "        series_to_array[i] = \" \".join([wnl.lemmatize(x) for x in nlp.word_tokenize(temp)])\n",
    "    return data\n",
    "\n",
    "X_final_text = prepareData(X[\"text\"])\n",
    "X_final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#extract word feature from data(convert to sparse matrix)\n",
    "vectorizer = CountVectorizer(max_features=440, stop_words=\"english\")\n",
    "X_sparse = vectorizer.fit_transform(X_final_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__', 'account', 'actually', 'ago', 'album', 'amas', 'amazing', 'american', 'amp', 'answer', 'app', 'apple', 'art', 'artist', 'artistoftheyear', 'ask', 'available', 'away', 'awesome', 'baby', 'bacon', 'bad', 'bc', 'beautiful', 'bed', 'believe', 'best', 'better', 'big', 'birthday', 'bit', 'bitch', 'black', 'blog', 'blue', 'body', 'bond', 'book', 'boy', 'break', 'bring', 'brother', 'build', 'building', 'business', 'buy', 'called', 'came', 'cancer', 'car', 'card', 'care', 'cat', 'cause', 'chance', 'change', 'channel', 'character', 'check', 'child', 'chill', 'city', 'class', 'click', 'club', 'come', 'coming', 'cool', 'costume', 'couldn', 'couple', 'cup', 'cut', 'cute', 'dad', 'damn', 'date', 'day', 'dead', 'deal', 'desk', 'did', 'didn', 'difference', 'different', 'digital', 'doe', 'doesn', 'dog', 'doing', 'don', 'dont', 'door', 'dream', 'drive', 'dude', 'early', 'eat', 'end', 'enjoy', 'episode', 'event', 'everydayiloveyou', 'excited', 'eye', 'face', 'facebook', 'fact', 'fall', 'family', 'fan', 'far', 'favorite', 'feel', 'feeling', 'fight', 'film', 'final', 'finally', 'follow', 'followed', 'follower', 'following', 'food', 'forevermore', 'forget', 'free', 'friday', 'friend', 'fuck', 'fucking', 'fun', 'funny', 'future', 'game', 'getting', 'girl', 'goal', 'god', 'going', 'gon', 'gone', 'good', 'got', 'great', 'group', 'gt', 'guy', 'ha', 'hair', 'half', 'halloween', 'hand', 'happen', 'happy', 'hard', 'harry_styles', 'hate', 'haven', 'having', 'head', 'health', 'hear', 'heard', 'heart', 'hell', 'hello', 'help', 'hey', 'hi', 'high', 'history', 'hit', 'hold', 'home', 'hope', 'hot', 'hour', 'house', 'http', 'human', 'idea', 'im', 'inside', 'internet', 'isn', 'issue', 'james', 'job', 'join', 'just', 'kid', 'kind', 'know', 'lady', 'later', 'latest', 'le', 'learn', 'leave', 'left', 'let', 'life', 'light', 'like', 'liked', 'line', 'link', 'listen', 'listening', 'literally', 'little', 'live', 'll', 'lmao', 'lol', 'long', 'look', 'looked', 'looking', 'lord', 'lost', 'lot', 'love', 'lt', 'make', 'making', 'man', 'matter', 'maybe', 'mean', 'meat', 'medium', 'meet', 'men', 'mind', 'minute', 'miss', 'mom', 'moment', 'monday', 'money', 'month', 'morning', 'movie', 'music', 'na', 'need', 'new', 'news', 'nice', 'nigga', 'night', 'number', 'october', 'office', 'oh', 'ok', 'okay', 'old', 'onedirection', 'online', 'open', 'order', 'parent', 'party', 'past', 'pay', 'people', 'perfect', 'person', 'phone', 'photo', 'pic', 'pick', 'picture', 'piece', 'place', 'plan', 'play', 'player', 'playing', 'pm', 'point', 'pop', 'post', 'power', 'ppl', 'premiere', 'pretty', 'price', 'probably', 'problem', 'product', 'pumpkin', 'pushawardslizquens', 'question', 'quote', 'read', 'ready', 'real', 'really', 'reason', 'red', 'remember', 'rest', 'right', 'rock', 'room', 'round', 'run', 'sad', 'said', 'sale', 'saturday', 'save', 'saw', 'say', 'saying', 'school', 'season', 'second', 'seeing', 'seen', 'service', 'set', 'share', 'shirt', 'shit', 'sign', 'single', 'sit', 'sleep', 'smile', 'social', 'song', 'soon', 'sorry', 'sound', 'space', 'special', 'spectre', 'st', 'star', 'start', 'started', 'state', 'stats', 'stay', 'step', 'stop', 'storage', 'store', 'story', 'street', 'struggle', 'student', 'stuff', 'super', 'support', 'sure', 'ta', 'taking', 'talk', 'talking', 'team', 'tech', 'tell', 'th', 'thank', 'thanks', 'thing', 'think', 'thinking', 'thought', 'ticket', 'time', 'today', 'told', 'tomorrow', 'tonight', 'took', 'tour', 'transforming', 'true', 'truth', 'try', 'trying', 'turn', 'tv', 'tweet', 'twitter', 'uk', 'unfollowed', 'unit', 'update', 'ur', 'use', 'used', 'using', 've', 'video', 'visit', 'voice', 'vote', 'voted', 'wa', 'wait', 'walk', 'walking', 'wan', 'want', 'wanted', 'watch', 'watching', 'water', 'way', 'weather', 'week', 'weekend', 'went', 'white', 'win', 'wish', 'woman', 'won', 'word', 'work', 'workbench', 'working', 'world', 'worst', 'worth', 'wrong', 'ya', 'yeah', 'year', 'yes', 'yesterday', 'youtube']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model accuracy : 0.6912796525225526\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'base_score': 0.5,\n",
       " 'booster': 'gbtree',\n",
       " 'colsample_bylevel': 1,\n",
       " 'colsample_bynode': 1,\n",
       " 'colsample_bytree': 1,\n",
       " 'gamma': 0,\n",
       " 'learning_rate': 0.001,\n",
       " 'max_delta_step': 0,\n",
       " 'max_depth': 8,\n",
       " 'min_child_weight': 1,\n",
       " 'missing': None,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': -1,\n",
       " 'nthread': None,\n",
       " 'objective': 'binary:logistic',\n",
       " 'random_state': 0,\n",
       " 'reg_alpha': 0,\n",
       " 'reg_lambda': 1,\n",
       " 'scale_pos_weight': 1,\n",
       " 'seed': None,\n",
       " 'silent': None,\n",
       " 'subsample': 1,\n",
       " 'verbosity': 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we have sparse matrix for these features\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#split train and test data\n",
    "X_full = X_sparse.toarray()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_full, y, test_size=0.15, random_state=23)\n",
    "\n",
    "model = XGBClassifier(learning_rate=0.001, max_depth=8, n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "print(\"model accuracy : {}\".format(model.score(X_test, y_test)))\n",
    "model.get_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbaseconda0ded40faa8164f549dbe7a32646a7b88"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
